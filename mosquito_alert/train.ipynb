{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-12T09:25:18.859738800Z",
     "start_time": "2023-10-12T09:25:12.228508500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torchsummary import summary\n",
    "import sys\n",
    "\n",
    "curdir = \"C:/Users/dinle/Code/AI/Aicrowd/mosquito_alert/\"\n",
    "sys.path.append(curdir + \"mnist\")\n",
    "\n",
    "from mnist import load_mnist\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de69aefe60d28bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-12T09:25:18.874741200Z",
     "start_time": "2023-10-12T09:25:18.860738700Z"
    }
   },
   "outputs": [],
   "source": [
    "# data_file = \"C:/Users/dinle/Code/AI/NodeLayer/dataset/mosquito/tensor_image.pt\"\n",
    "# all_data = torch.load(data_file)\n",
    "# all_data = all_data[:5000]\n",
    "# torch.save(all_data, \"C:/Users/dinle/Code/AI/Aicrowd/mosquitoAlert/tensor_image_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1808f22a0157ff84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-12T09:25:18.916744600Z",
     "start_time": "2023-10-12T09:25:18.874741200Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_data(data_name, cut=5000, tr_lr=0.8):\n",
    "    if data_name == 'mnist':\n",
    "        (tr_data, tr_lb), (ts_data, ts_lb) = load_mnist(normalize=True)\n",
    "        tr_data = tr_data.reshape(-1,1,28,28)\n",
    "        ts_data = ts_data.reshape(-1,1,28,28)\n",
    "        tr_data = torch.tensor(tr_data)\n",
    "        tr_lb = torch.tensor(tr_lb)\n",
    "        ts_data = torch.tensor(ts_data)\n",
    "        ts_lb = torch.tensor(ts_lb)\n",
    "    elif data_name == 'mosquito':\n",
    "        # DATA\n",
    "        data_file = curdir + \"tensor_image.pt\"\n",
    "        # .pt 파일을 엽니다.\n",
    "        all_data = torch.load(data_file)\n",
    "        all_data = torch.tensor(all_data[:cut])\n",
    "        \n",
    "        # label\n",
    "        lb_file = curdir + \"phase2_train_v0.csv\"\n",
    "        df = pd.read_csv(lb_file)\n",
    "        all_lb = df['class_label']\n",
    "        class_num = {\n",
    "            \"aegypti\":      0,\n",
    "            \"albopictus\":   1,\n",
    "            \"anopheles\":    2,\n",
    "            \"culex\":        3,\n",
    "            \"culiseta\":     4,\n",
    "            \"japonicus/koreicus\": 5\n",
    "        }\n",
    "        all_lb = all_lb.map(class_num)\n",
    "        all_lb = torch.tensor(all_lb[:cut])\n",
    "        tr_cut = int(cut * 0.8)\n",
    "        # 분리\n",
    "        tr_data = all_data[:tr_cut]\n",
    "        tr_lb = all_lb[:tr_cut]\n",
    "        ts_data = all_data[tr_cut:]\n",
    "        ts_lb = all_lb[tr_cut:]\n",
    "    else:\n",
    "        assert 'No data'\n",
    "        \n",
    "    print(tr_data.shape, tr_lb.shape, ts_data.shape, ts_lb.shape)\n",
    "\n",
    "    tr_dataset = TensorDataset(tr_data, tr_lb)\n",
    "    ts_dataset = TensorDataset(ts_data, ts_lb)\n",
    "    train_loader = DataLoader(tr_dataset,batch_size=batch_size)\n",
    "    test_loader = DataLoader(ts_dataset,batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, test_loader, (tuple(tr_data.shape[1:]), int(max(tr_lb))+1)\n",
    "\n",
    "def calculate_accuracy(output, target):\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    correct = (predicted == target).sum().item()\n",
    "    accuracy = correct / target.size(0)\n",
    "    return accuracy\n",
    "\n",
    "def imshow(ts_img):\n",
    "    np_img = ts_img.numpy()\n",
    "    np_img = (np_img * 225).astype(np.uint8)\n",
    "    plt.imshow(np_img.transpose(1,2,0))\n",
    "    plt.show()\n",
    "\n",
    "def show_result(img, y, t):\n",
    "    _, p = torch.max(y, 1)\n",
    "    correct = (p == t).sum().item()\n",
    "    accuracy = correct / t.size(0)\n",
    "    imshow(img[0])\n",
    "    print(p)\n",
    "    print(t)\n",
    "    print(correct)\n",
    "    print(accuracy)\n",
    "\n",
    "def smooth_curve(x):\n",
    "    window_len = 11\n",
    "    s = np.r_[x[window_len-1:0:-1], x, x[-1:-window_len:-1]]\n",
    "    w = np.kaiser(window_len, 2)\n",
    "    y = np.convolve(w/w.sum(), s, mode='valid')\n",
    "    return y[5:len(y)-5]\n",
    "\n",
    "def plot(label, datas, *y_lim):\n",
    "    # datas = smooth_curve(datas)\n",
    "    plt.plot(datas, markevery=50)\n",
    "    plt.xlabel(\"iter\")\n",
    "    plt.ylabel(label)\n",
    "    if y_lim:\n",
    "        plt.ylim(y_lim)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8135d0208667f83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-12T09:25:18.918739900Z",
     "start_time": "2023-10-12T09:25:18.890738300Z"
    }
   },
   "outputs": [],
   "source": [
    "data_name = 'mosquito'\n",
    "# data_name = 'mnist'\n",
    "\n",
    "cut = 4000\n",
    "tr_lr = 0.8\n",
    "batch_size = 40\n",
    "\n",
    "whole_epoch = 1\n",
    "tr_print = 1\n",
    "ts_print = max(1, int(tr_print * (1 - tr_lr)))\n",
    "\n",
    "learning_rate = 0.001\n",
    "base_fn = 32\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32d9ec7f92f07509",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-12T09:25:29.518334400Z",
     "start_time": "2023-10-12T09:25:18.906732900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dinle\\AppData\\Local\\Temp\\ipykernel_27668\\4077969317.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  all_data = torch.tensor(all_data[:cut])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3200, 3, 224, 224]) torch.Size([3200]) torch.Size([800, 3, 224, 224]) torch.Size([800])\n",
      "Train with GPU\n",
      "Input Data Shape: (3, 224, 224)\n",
      "Class Number: 6\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader, size = set_data(data_name, cut, tr_lr)\n",
    "input_size = size[0]\n",
    "lb_size = size[1]\n",
    "# input_size = (3, 224, 224)\n",
    "# lb_size = 6\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Train with GPU\")\n",
    "else:\n",
    "    print(\"Train with CPU\")\n",
    "print(f\"Input Data Shape: {input_size}\\nClass Number: {lb_size}\")\n",
    "# # 학습용 이미지를 무작위로 가져오기\n",
    "# for i,[image,label] in enumerate(train_loader):\n",
    "#     # 이미지 보여주기\n",
    "#     imshow(torchvision.utils.make_grid(image))\n",
    "#     \n",
    "#     # 정답(label) 출력\n",
    "#     print(' '.join('%5s' % label[i] for i in range(batch_size)))\n",
    "#     if 1<i:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b08baa62b3c4bef3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-12T09:25:29.560841700Z",
     "start_time": "2023-10-12T09:25:29.527330900Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv_2_block(in_ch,out_ch,kernel_size,stride,padding,pool_size,data_size):\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(in_ch,out_ch,kernel_size,stride,padding),\n",
    "        nn.ReLU(),\n",
    "        # nn.MaxPool2d(pool_size,pool_size),\n",
    "        nn.Conv2d(out_ch,out_ch,kernel_size,stride,padding),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(pool_size,pool_size),\n",
    "        nn.BatchNorm2d(out_ch)\n",
    "    )\n",
    "    output_size = ((data_size[1] - kernel_size + 2 * padding) // stride) + 1\n",
    "    # output_size = (output_size - pool_size) // pool_size + 1\n",
    "    output_size = ((output_size- kernel_size + 2 * padding) // stride) + 1\n",
    "    output_size = (output_size - pool_size) // pool_size + 1\n",
    "    \n",
    "    data_size[0] = out_ch\n",
    "    data_size[1] = output_size\n",
    "    data_size[2] = output_size\n",
    "    \n",
    "    return model\n",
    "\n",
    "def conv_3_block(in_ch,out_ch,kernel_size,stride,padding,pool_size,data_size):\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(in_ch,out_ch,kernel_size,stride,padding),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_ch,out_ch,kernel_size,stride,padding),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_ch,out_ch,kernel_size,stride,padding),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2,2),\n",
    "        nn.BatchNorm2d(out_ch)\n",
    "    )\n",
    "    data_size[0] = out_ch\n",
    "    output_size = ((data_size[1] - kernel_size + 2 * padding) // stride) + 1\n",
    "    output_size = ((output_size - kernel_size + 2 * padding) // stride) + 1\n",
    "    output_size = ((output_size - kernel_size + 2 * padding) // stride) + 1\n",
    "    output_size = (output_size - pool_size) // pool_size + 1\n",
    "    \n",
    "    data_size[0] = out_ch\n",
    "    data_size[1] = output_size\n",
    "    data_size[2] = output_size\n",
    "    \n",
    "    return model\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self, input_size, lb_size, base_fn=64, kernel_size=3, stride=1, padding=1, pool_size=2):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.data_size = list(input_size).copy()\n",
    "        self.feature = nn.Sequential(\n",
    "            conv_2_block(input_size[0], base_fn, kernel_size, stride, padding, pool_size, self.data_size),\n",
    "            conv_2_block(self.data_size[0], 2*base_fn, kernel_size, stride, padding, pool_size, self.data_size),\n",
    "            conv_3_block(self.data_size[0], 4*base_fn, kernel_size, stride, padding, pool_size, self.data_size), \n",
    "            conv_3_block(self.data_size[0], 8*base_fn, kernel_size, stride, padding, pool_size, self.data_size), \n",
    "            conv_3_block(self.data_size[0], 8*base_fn, kernel_size, stride, padding, pool_size, self.data_size), \n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(self.data_size[0] * self.data_size[1] * self.data_size[2],4096),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(4096, 1000),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(1000, lb_size),\n",
    "        )\n",
    "        \n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        #         init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(x.shape)\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "    \n",
    "class VGG11(nn.Module):\n",
    "    def __init__(self, input_size, lb_size, base_fn=64, kernel_size=3, stride=1, padding=1, pool_size=2):\n",
    "        super(VGG11, self).__init__()\n",
    "        self.data_size = list(input_size).copy()\n",
    "        self.feature = nn.Sequential(\n",
    "            conv_2_block(input_size[0], base_fn, kernel_size, stride, padding, pool_size, self.data_size), \n",
    "            conv_3_block(self.data_size[0], 2*base_fn, kernel_size, stride, padding, pool_size, self.data_size), \n",
    "            conv_3_block(self.data_size[0], 4*base_fn, kernel_size, stride, padding, pool_size, self.data_size), \n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(self.data_size[0] * self.data_size[1] * self.data_size[2],4096),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(4096, 1000),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(1000, lb_size),\n",
    "        )\n",
    "        \n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        #         init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(x.shape)\n",
    "        x = self.fc_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23d4126349d81d83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-12T09:25:37.085160300Z",
     "start_time": "2023-10-12T09:25:29.535845200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG11(\n",
      "  (feature): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU()\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU()\n",
      "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): ReLU()\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU()\n",
      "      (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): ReLU()\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (fc_layer): Sequential(\n",
      "    (0): Linear(in_features=100352, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=1000, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 224, 224]             896\n",
      "              ReLU-2         [-1, 32, 224, 224]               0\n",
      "            Conv2d-3         [-1, 32, 224, 224]           9,248\n",
      "              ReLU-4         [-1, 32, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 32, 112, 112]               0\n",
      "       BatchNorm2d-6         [-1, 32, 112, 112]              64\n",
      "            Conv2d-7         [-1, 64, 112, 112]          18,496\n",
      "              ReLU-8         [-1, 64, 112, 112]               0\n",
      "            Conv2d-9         [-1, 64, 112, 112]          36,928\n",
      "             ReLU-10         [-1, 64, 112, 112]               0\n",
      "           Conv2d-11         [-1, 64, 112, 112]          36,928\n",
      "             ReLU-12         [-1, 64, 112, 112]               0\n",
      "        MaxPool2d-13           [-1, 64, 56, 56]               0\n",
      "      BatchNorm2d-14           [-1, 64, 56, 56]             128\n",
      "           Conv2d-15          [-1, 128, 56, 56]          73,856\n",
      "             ReLU-16          [-1, 128, 56, 56]               0\n",
      "           Conv2d-17          [-1, 128, 56, 56]         147,584\n",
      "             ReLU-18          [-1, 128, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 56, 56]         147,584\n",
      "             ReLU-20          [-1, 128, 56, 56]               0\n",
      "        MaxPool2d-21          [-1, 128, 28, 28]               0\n",
      "      BatchNorm2d-22          [-1, 128, 28, 28]             256\n",
      "           Linear-23                 [-1, 4096]     411,045,888\n",
      "             ReLU-24                 [-1, 4096]               0\n",
      "           Linear-25                 [-1, 1000]       4,097,000\n",
      "             ReLU-26                 [-1, 1000]               0\n",
      "           Linear-27                    [-1, 6]           6,006\n",
      "================================================================\n",
      "Total params: 415,620,862\n",
      "Trainable params: 415,620,862\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 114.92\n",
      "Params size (MB): 1585.47\n",
      "Estimated Total Size (MB): 1700.96\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# VGG\n",
    "model = VGG11(input_size, lb_size, base_fn).to(device)\n",
    "print(model)\n",
    "summary(model, input_size, device=device.type)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6026f8f80d37a1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-12T09:25:37.087160Z"
    },
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], 40Batch [1/80], Loss: 1.8126, Accuracy: 0.1250\n",
      "Epoch [1/1], 40Batch [2/80], Loss: 28.3161, Accuracy: 0.3750\n",
      "Epoch [1/1], 40Batch [3/80], Loss: 45.6050, Accuracy: 0.4500\n",
      "Epoch [1/1], 40Batch [4/80], Loss: 47.3468, Accuracy: 0.5000\n",
      "Epoch [1/1], 40Batch [5/80], Loss: 44.5528, Accuracy: 0.4250\n",
      "Epoch [1/1], 40Batch [6/80], Loss: 6.6176, Accuracy: 0.5750\n",
      "Epoch [1/1], 40Batch [7/80], Loss: 19.0411, Accuracy: 0.4500\n",
      "Epoch [1/1], 40Batch [8/80], Loss: 10.0751, Accuracy: 0.4000\n",
      "Epoch [1/1], 40Batch [9/80], Loss: 7.8644, Accuracy: 0.0250\n",
      "Epoch [1/1], 40Batch [10/80], Loss: 4.9134, Accuracy: 0.2500\n",
      "Epoch [1/1], 40Batch [11/80], Loss: 5.7274, Accuracy: 0.4500\n",
      "Epoch [1/1], 40Batch [12/80], Loss: 2.7596, Accuracy: 0.3000\n",
      "Epoch [1/1], 40Batch [13/80], Loss: 3.6851, Accuracy: 0.2750\n",
      "Epoch [1/1], 40Batch [14/80], Loss: 2.8786, Accuracy: 0.4250\n",
      "Epoch [1/1], 40Batch [15/80], Loss: 1.7427, Accuracy: 0.4000\n",
      "Epoch [1/1], 40Batch [16/80], Loss: 1.8135, Accuracy: 0.3000\n",
      "Epoch [1/1], 40Batch [17/80], Loss: 3.2095, Accuracy: 0.3500\n",
      "Epoch [1/1], 40Batch [18/80], Loss: 1.3849, Accuracy: 0.4000\n",
      "Epoch [1/1], 40Batch [19/80], Loss: 1.1250, Accuracy: 0.4750\n",
      "Epoch [1/1], 40Batch [20/80], Loss: 1.1210, Accuracy: 0.4500\n",
      "Epoch [1/1], 40Batch [21/80], Loss: 1.3539, Accuracy: 0.4500\n",
      "Epoch [1/1], 40Batch [22/80], Loss: 0.9957, Accuracy: 0.4750\n",
      "Epoch [1/1], 40Batch [23/80], Loss: 1.3672, Accuracy: 0.5500\n",
      "Epoch [1/1], 40Batch [24/80], Loss: 1.2179, Accuracy: 0.3500\n",
      "Epoch [1/1], 40Batch [25/80], Loss: 1.7365, Accuracy: 0.5000\n",
      "Epoch [1/1], 40Batch [26/80], Loss: 0.8440, Accuracy: 0.6000\n",
      "Epoch [1/1], 40Batch [27/80], Loss: 1.3430, Accuracy: 0.5000\n",
      "Epoch [1/1], 40Batch [28/80], Loss: 1.1180, Accuracy: 0.5250\n",
      "Epoch [1/1], 40Batch [29/80], Loss: 1.0049, Accuracy: 0.4250\n",
      "Epoch [1/1], 40Batch [30/80], Loss: 1.5256, Accuracy: 0.3000\n",
      "Epoch [1/1], 40Batch [31/80], Loss: 1.3334, Accuracy: 0.4000\n",
      "Epoch [1/1], 40Batch [32/80], Loss: 1.0025, Accuracy: 0.5000\n",
      "Epoch [1/1], 40Batch [33/80], Loss: 1.3849, Accuracy: 0.5250\n",
      "Epoch [1/1], 40Batch [34/80], Loss: 1.1322, Accuracy: 0.5250\n",
      "Epoch [1/1], 40Batch [35/80], Loss: 1.2361, Accuracy: 0.3000\n",
      "Epoch [1/1], 40Batch [36/80], Loss: 1.0269, Accuracy: 0.4750\n",
      "Epoch [1/1], 40Batch [37/80], Loss: 1.1908, Accuracy: 0.4000\n",
      "Epoch [1/1], 40Batch [38/80], Loss: 1.1907, Accuracy: 0.4750\n",
      "Epoch [1/1], 40Batch [39/80], Loss: 1.5896, Accuracy: 0.4000\n",
      "Epoch [1/1], 40Batch [40/80], Loss: 1.2257, Accuracy: 0.3500\n",
      "Epoch [1/1], 40Batch [41/80], Loss: 1.2572, Accuracy: 0.3500\n",
      "Epoch [1/1], 40Batch [42/80], Loss: 1.0350, Accuracy: 0.4500\n",
      "Epoch [1/1], 40Batch [43/80], Loss: 0.9599, Accuracy: 0.4750\n",
      "Epoch [1/1], 40Batch [44/80], Loss: 0.9124, Accuracy: 0.4250\n",
      "Epoch [1/1], 40Batch [45/80], Loss: 1.1113, Accuracy: 0.3750\n",
      "Epoch [1/1], 40Batch [46/80], Loss: 0.8867, Accuracy: 0.3750\n",
      "Epoch [1/1], 40Batch [47/80], Loss: 1.1508, Accuracy: 0.4000\n",
      "Epoch [1/1], 40Batch [48/80], Loss: 1.1796, Accuracy: 0.4500\n",
      "Epoch [1/1], 40Batch [49/80], Loss: 1.1218, Accuracy: 0.4750\n",
      "Epoch [1/1], 40Batch [50/80], Loss: 1.2880, Accuracy: 0.5000\n",
      "Epoch [1/1], 40Batch [51/80], Loss: 0.9355, Accuracy: 0.6000\n",
      "Epoch [1/1], 40Batch [52/80], Loss: 1.2465, Accuracy: 0.4250\n",
      "Epoch [1/1], 40Batch [53/80], Loss: 1.2113, Accuracy: 0.4250\n"
     ]
    }
   ],
   "source": [
    "tr_loss_list = []\n",
    "tr_acc_list = []\n",
    "for i in range(whole_epoch):\n",
    "    for j,[image,label] in enumerate(train_loader):\n",
    "        x = image.to(device)\n",
    "        t = label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y = model.forward(x)\n",
    "        loss = loss_func(y, t)\n",
    "        tr_loss_list.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        accuracy = calculate_accuracy(y, t) \n",
    "        tr_acc_list.append(accuracy)\n",
    "        if j %  tr_print ==0 or j == (len(train_loader)-1):\n",
    "            print(f'Epoch [{i + 1}/{whole_epoch}], {batch_size}Batch [{j + 1}/{len(train_loader)}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}')\n",
    "            # show_result(image, y, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4b04419c218406",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "ts_loss_list = []\n",
    "ts_acc_list = []\n",
    "\n",
    "for j,[image,label] in enumerate(test_loader):\n",
    "    x = image.to(device)\n",
    "    t = label.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    y = model.forward(x)\n",
    "    loss = loss_func(y, t)\n",
    "    ts_loss_list.append(loss.cpu().detach().numpy())\n",
    "\n",
    "    accuracy = calculate_accuracy(y, t) \n",
    "    ts_acc_list.append(accuracy)\n",
    "    if j % ts_print ==0 or j == (len(test_loader)-1):\n",
    "        print(f'{batch_size}Batch [{j + 1}/{len(test_loader)}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}')\n",
    "        # show_result(image, y, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f21629c960b68ac",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "plot('train loss', tr_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c152daae02cf922",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "plot('train acc', tr_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60c7152c423eacc",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "plot('test loss', ts_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b163b2129829a066",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "plot('test acc', ts_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aec4d64c4a8a335",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), curdir + 'model_weights.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
